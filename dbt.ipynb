{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bac680-6df6-48be-b516-0cb9ff811d52",
   "metadata": {},
   "source": [
    "# DBT CLI on Cloud Run\n",
    "\n",
    "This notebook demonstrates running Dataform using its command line interface (CLI) application, on Cloud Run. The steps are:\n",
    "\n",
    "* Installation of local software\n",
    "* Google Cloud configuration (permissions, enabling APIs)\n",
    "* Local set-up of DBT and definitions of transformations\n",
    "* Generation of container to execute the DBT application using Cloud Build\n",
    "* Deploy the container to Cloud Run\n",
    "* Execute the container in a Cloud Run Job\n",
    "\n",
    "### Not discussed / out of scope\n",
    "\n",
    "* Integrate the Cloud Build to update the container in a CI/CD process (we have not included the trigger)\n",
    "* Orchestation of the Cloud Run Job: What triggers it?\n",
    "\n",
    "For the orchestration, the Cloud Run Job is triggered by an HTTP request. This HTTP request can be launched from Composer (managed Airflow) as part of a DAG, Cloud Scheduler as part of cron-like schedule, or through some other mechanism that could run `gcloud beta run jobs execute`. \n",
    "\n",
    "### Installation of local software\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e29b08-1d88-4424-a050-20412ca1a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade dbt-core dbt-bigquery\n",
    "!echo \"export PATH=$(python -m site --user-base)/bin:$PATH\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33320ca5-6743-4e4a-acde-81f5fb8da0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "dbt --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d723d-0bb6-49a3-b7d0-d50323e9f7bb",
   "metadata": {},
   "source": [
    "### Enable Google Cloud APIs that we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9302b3-c750-4803-bb05-f38f53ced1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com cloudbuild.googleapis.com datacatalog.googleapis.com datalineage.googleapis.com run.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f79090-609e-4911-bc2a-3c5e894d3d97",
   "metadata": {},
   "source": [
    "### Retrieve Google Cloud log-in credentials\n",
    "\n",
    "Since this command requires some interaction, we will run it in a terminal: `gcloud auth login`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98650b8-b625-45e1-9f09-72045afbdb60",
   "metadata": {},
   "source": [
    "### DBT project set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4429b2-d5c2-4443-bcc8-8b57614fe0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "mkdir -p ~/.dbt\n",
    "\n",
    "cat << EOF > ~/.dbt/profiles.yml\n",
    "dbt_proj_dir: # this needs to match the name of the project that you will init later\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: bigquery\n",
    "      method: oauth\n",
    "      project: $(gcloud config get project)\n",
    "      dataset: dbt_demo\n",
    "      threads: 1\n",
    "      timeout_seconds: 300\n",
    "      location: europe-west4\n",
    "      priority: interactive\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48667b98-cb31-4f45-a474-1615c3c6933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.dbt/profiles.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f5451-5317-4dc7-92dc-549e1fd89108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "cd dbt_proj_dir\n",
    "dbt debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9164fa-4230-49f2-bd81-e9523d9563ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "yes N | dbt init dbt_proj_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1657807-9b92-497f-8702-e104dec312e1",
   "metadata": {},
   "source": [
    "If this works, copy over the profile config to the current project directory, so we include it in the container later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c052-9eea-433b-83e3-839152309269",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/.dbt/profiles.yml dbt_proj_dir/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c8c12-d2e0-4420-a1fb-421579d2f92a",
   "metadata": {},
   "source": [
    "### Prepare some sample data in BigQuery\n",
    "\n",
    "Start by creating the datasets, one for the raw data, and one for the data transformed with Dataform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be7f19-61cc-4cf0-8b90-3d43fe933a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=europe-west4 mk --dataset ${PROJECT_ID}.prod_raw\n",
    "!bq --location=europe-west4 mk --dataset ${PROJECT_ID}.dbt_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d01b37-906a-46e7-87c9-b0075a4323dd",
   "metadata": {},
   "source": [
    "Upload the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6325d-07f7-4f65-8493-2c3ca313d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq load --source_format=PARQUET prod_raw.sales_data data/sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cd8c1-b5d8-4658-9f3f-4e4a5823465f",
   "metadata": {},
   "source": [
    "### DBT definitions and transformations\n",
    "\n",
    "First we declare the source table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65278294-1524-4fee-826b-4a45c47eb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dbt_proj_dir/models/sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb51d9-c93d-4c26-a8bf-d62001946536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dbt_proj_dir/models/sales/sources.yml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: prod_raw\n",
    "    tables:\n",
    "      - name: sales_data\n",
    "        description: \"Ingested sales data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d436eb4-e2a1-4373-9aa5-1c34e4b8e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dbt_proj_dir/models/sales/sales_agg.sql\n",
    "{{\n",
    "  config(\n",
    "    materialized = \"table\"\n",
    "  )\n",
    "}}\n",
    "\n",
    "WITH daily_orders AS (\n",
    "    SELECT\n",
    "      DATE(orderdate) AS order_date, \n",
    "      PRODUCTLINE AS product_line,\n",
    "      ROUND(SUM(SALES), 1) AS sales_value\n",
    "    FROM\n",
    "      {{ source(\"prod_raw\", \"sales_data\") }}\n",
    "    WHERE\n",
    "      STATUS = \"Shipped\"\n",
    "    GROUP BY\n",
    "      1,\n",
    "      2\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    order_date, \n",
    "    product_line, \n",
    "    sales_value, \n",
    "    ROUND(SUM(sales_value) OVER (ORDER BY DATE(order_date) ROWS BETWEEN 7 PRECEDING AND CURRENT ROW  ), 1) AS rolling_average\n",
    "FROM daily_orders\n",
    "ORDER BY 1 DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3651bb-0d72-4964-9d09-61e6dbe5e1e8",
   "metadata": {},
   "source": [
    "### Test running the transformations\n",
    "\n",
    "The expected output is:\n",
    "\n",
    "```\n",
    "17:45:56  Running with dbt=1.3.1\n",
    "\n",
    "17:45:56  Found 2 models, 0 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics\n",
    "17:45:56  \n",
    "17:45:57  Concurrency: 1 threads (target='dev')\n",
    "17:45:57  \n",
    "17:45:57  1 of 2 START sql table model dbt_demo.sales_agg ................................ [RUN]\n",
    "17:45:59  1 of 2 OK created sql table model dbt_demo.sales_agg ........................... [CREATE TABLE (560.0 rows, 943.4 KB processed) in 2.37s]\n",
    "17:45:59  2 of 2 START sql table model dbt_demo.sales_agg-checkpoint ..................... [RUN]\n",
    "17:46:01  2 of 2 OK created sql table model dbt_demo.sales_agg-checkpoint ................ [CREATE TABLE (560.0 rows, 943.4 KB processed) in 2.12s]\n",
    "17:46:01  \n",
    "17:46:01  Finished running 2 table models in 0 hours 0 minutes and 4.85 seconds (4.85s).\n",
    "17:46:01  \n",
    "17:46:01  Completed successfully\n",
    "17:46:01  \n",
    "17:46:01  Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170a20b-fc42-4583-9aae-eaf540320487",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dbt run --project-dir dbt_proj_dir/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae6499-5df7-4602-9f47-361f8364c30e",
   "metadata": {},
   "source": [
    "### Build the container to run on Cloud Run\n",
    "\n",
    "The container will have the Dataform application and our transformation code.\n",
    "\n",
    "First we create a repository in Artifact Registry to store the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2c408-f7a1-4d48-b5f8-217b64ff4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set artifacts/location europe-west4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f5163-baed-4f1d-ab57-8764700dcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create dbt --repository-format=docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431a94d-f1b5-453d-8aaf-c562522b2e3d",
   "metadata": {},
   "source": [
    "Define a `Dockerfile` to generate the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7fba9-9a65-4e7b-923d-a30907651042",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dbt_proj_dir/Dockerfile\n",
    "# Generic Dockerfile from https://github.com/dbt-labs/dbt-core/blob/main/docker/Dockerfile\n",
    "\n",
    "# Top level build args\n",
    "ARG build_for=linux/amd64\n",
    "\n",
    "##\n",
    "# base image (abstract)\n",
    "##\n",
    "FROM --platform=$build_for python:3.10.7-slim-bullseye as base\n",
    "\n",
    "# N.B. The refs updated automagically every release via bumpversion\n",
    "# N.B. dbt-postgres is currently found in the core codebase so a value of dbt-core@<some_version> is correct\n",
    "\n",
    "ARG dbt_core_ref=dbt-core@v1.4.0b1\n",
    "ARG dbt_bigquery_ref=dbt-bigquery@v1.4.0b1\n",
    "# special case args\n",
    "ARG dbt_spark_version=all\n",
    "ARG dbt_third_party\n",
    "\n",
    "# System setup\n",
    "RUN apt-get update \\\n",
    "  && apt-get dist-upgrade -y \\\n",
    "  && apt-get install -y --no-install-recommends \\\n",
    "    git \\\n",
    "    ssh-client \\\n",
    "    software-properties-common \\\n",
    "    make \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    libpq-dev \\\n",
    "  && apt-get clean \\\n",
    "  && rm -rf \\\n",
    "    /var/lib/apt/lists/* \\\n",
    "    /tmp/* \\\n",
    "    /var/tmp/*\n",
    "\n",
    "# Env vars\n",
    "ENV PYTHONIOENCODING=utf-8\n",
    "ENV LANG=C.UTF-8\n",
    "\n",
    "# Update python\n",
    "RUN python -m pip install --upgrade pip setuptools wheel --no-cache-dir\n",
    "\n",
    "##\n",
    "# dbt-core\n",
    "##\n",
    "FROM base as dbt-core\n",
    "RUN python -m pip install --no-cache-dir \"git+https://github.com/dbt-labs/${dbt_core_ref}#egg=dbt-core&subdirectory=core\"\n",
    "\n",
    "##\n",
    "# dbt-bigquery\n",
    "##\n",
    "FROM base as dbt-bigquery\n",
    "RUN python -m pip install --no-cache-dir \"git+https://github.com/dbt-labs/${dbt_bigquery_ref}#egg=dbt-bigquery\"\n",
    "\n",
    "##\n",
    "# dbt-third-party\n",
    "##\n",
    "FROM dbt-core as dbt-third-party\n",
    "RUN python -m pip install --no-cache-dir \"${dbt_third_party}\"\n",
    "\n",
    "##\n",
    "# dbt-all\n",
    "##\n",
    "FROM base as dbt-all\n",
    "RUN apt-get update \\\n",
    "  && apt-get dist-upgrade -y \\\n",
    "  && apt-get install -y --no-install-recommends \\\n",
    "    python-dev \\\n",
    "    libsasl2-dev \\\n",
    "    gcc \\\n",
    "    unixodbc-dev \\\n",
    "  && apt-get clean \\\n",
    "  && rm -rf \\\n",
    "    /var/lib/apt/lists/* \\\n",
    "    /tmp/* \\\n",
    "    /var/tmp/*\n",
    "  RUN python -m pip install --no-cache \"git+https://github.com/dbt-labs/${dbt_bigquery_ref}#egg=dbt-bigquery\"\n",
    "\n",
    "\n",
    "# Set working directory\n",
    "ENV DBT_DIR /dbt/\n",
    "WORKDIR $DBT_DIR\n",
    "\n",
    "# Copy files to the image\n",
    "COPY . $DBT_DIR\n",
    "\n",
    "# Install Dbt deps\n",
    "RUN dbt deps\n",
    "\n",
    "# Run dbt\n",
    "ENTRYPOINT [\"dbt\", \"run\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae86802-018b-4dc9-a880-64d583a3d711",
   "metadata": {},
   "source": [
    "Here is a configuration file for Cloud Build to run the Docker file. We don't strictly need this, but it gives us more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82639e6-d54d-4e98-9024-29f25d696e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dbt_proj_dir/cloudbuild.yaml\n",
    "steps:\n",
    "- name: gcr.io/cloud-builders/docker\n",
    "  id: Build DBT image\n",
    "  env: \n",
    "    - 'DOCKER_BUILDKIT=1'\n",
    "  args: [\n",
    "      'build',\n",
    "      '-t', 'europe-west4-docker.pkg.dev/${PROJECT_ID}/dbt/dbt-demo',\n",
    "      '--cache-from', 'europe-west4-docker.pkg.dev/${PROJECT_ID}/dbt/dbt-demo:latest',\n",
    "      '.'\n",
    "    ]\n",
    "\n",
    "- name: gcr.io/cloud-builders/docker\n",
    "  id: Push DBT image to Artifact Registry\n",
    "  args: [\n",
    "      'push',\n",
    "      'europe-west4-docker.pkg.dev/${PROJECT_ID}/dbt/dbt-demo:latest'\n",
    "    ]\n",
    "\n",
    "options:\n",
    "  logging: CLOUD_LOGGING_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60991422-7ef7-45f4-bb02-833ffcae19b4",
   "metadata": {},
   "source": [
    "Submit the build to Cloud Build to generate the container and push it to Artifact Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225c2ae-5524-48e7-9887-e72aa9ed4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit dbt_proj_dir --config=dbt_proj_dir/cloudbuild.yaml --region=europe-west4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf3824-a8a6-4aed-ba08-488353b0afd6",
   "metadata": {},
   "source": [
    "### Test the container\n",
    "\n",
    "Now you can test it:\n",
    "    \n",
    "```\n",
    "docker pull europe-west4-docker.pkg.dev/${PROJECT_ID}/dataform/dataform-demo:latest\n",
    "docker run europe-west4-docker.pkg.dev/${PROJECT_ID}/dataform/dataform-demo:latest\n",
    "```\n",
    "\n",
    "The output should be as before when you tested running the transformations.\n",
    "\n",
    "### Create a Cloud Run Job for serverless execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1abc93-efe1-4321-9a79-d832ea0b2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = os.popen('gcloud config get project').read()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675fa4e-aab7-4d4d-bf46-09ec5c7a8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta run jobs create dbt-demo --image europe-west4-docker.pkg.dev/$PROJECT_ID/dbt/dbt-demo:latest --region europe-west4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de197b3d-4692-469e-8e5e-69e9f96edc7e",
   "metadata": {},
   "source": [
    "Run the Cloud Run job, which will run the Dataform transformations and recreate the aggregated table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e97469-bd51-4f65-bf72-1730218f60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta run jobs execute dbt-demo --region europe-west4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a21e8-b4d3-4fac-92e0-c84064112cd4",
   "metadata": {},
   "source": [
    "### CI/CD workflow using Github triggers\n",
    "\n",
    "See the [Cloud Build documentation](https://cloud.google.com/build/docs/automating-builds/github/build-repos-from-github#gcloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39437c3e-a8d3-484b-8c10-ef6edaaee923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
